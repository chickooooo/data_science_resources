
- a group of predictors is called an ensemble
- an ensemble of decision trees is called a random forest

- hard voting: picking the prediction with the highest number of votes
- soft voting: combining the probabilities of each prediction from each
               model and picking the prediction with highest average probability
- soft voting often achieves higher performance because it gives more weight to
  highly confident votes

- weak learner: model not able to generalise well and giving low accuracy
- strong learner: model giving high accuracy

- law of large numbers: as a sample size grows, it's mean gets closer to the average of whole population

- ensemble works best when the predictors are as independent from one another as possible
- one way to get diverse classifiers is to train them using very different algorithms



- bagging:
      -- bootstrap aggregation
      -- same algorithm is used for every predictor
      -- a random sample of data is selected from training set with replacement
      -- individual datapoints can be selected more than once
      -- the aggregating function for classification is statistical mode
      -- and for regression is statistical mean

- pasting: same as bagging but without replacement
- pasting should be preferred when you have large dataset as each predictor
  will be trained on independent data

- BaggingClassifier automatically performs soft voting if the predictor can predict probabilities

- out of bag evaluation (oob): evaluating each predictor of a bagging ensemble with
                               the leftover data that was not used for training

- random patches: sampling both training instances and features
- random subspaces: sampling only features
- bagging: sampling only instances



- random forest:
      -- bagging classifier with estimator = DecisionTree and max_samples = 1.0
      -- searches for the best feature to split from a random subset of features
      -- has better tree diversity than BaggingClassifier

- predictions made by random forest can be calculationally proven but are hard to interpret -> black box model

- extra trees:
      -- EXTremely RAndomized trees
      -- random forest does optimum split at each feature and then selects the feature with max information gain
      -- extra trees does random split at each feature and ...
      -- adds even more diversity to the ensemble

- 'feature_importances_' attribute of a tree like algorithm can tell us the importance
  of each feature in decision making



- boosting:
      -- train predictors sequentially, each predictor trying to correct its predecessor

- adaboost:
      -- adaptive boosting
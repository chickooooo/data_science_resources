
- a group of predictors is called an ensemble
- an ensemble of decision trees is called a random forest

- hard voting: picking the prediction with the highest number of votes
- soft voting: combining the probabilities of each prediction from each
               model and picking the prediction with highest average probability
- soft voting often achieves higher performance because it gives more weight to
  highly confident votes

- weak learner: model not able to generalise well and giving low accuracy
- strong learner: model giving high accuracy

- law of large numbers: as a sample size grows, it's mean gets closer to the average of whole population

- ensemble works best when the predictors are as independent from one another as possible
- one way to get diverse classifiers is to train them using very different algorithms



- bagging:
      -- bootstrap aggregation
      -- same algorithm is used for every predictor
      -- a random sample of data is selected from training set with replacement
      -- individual datapoints can be selected more than once
      -- the aggregating function for classification is statistical mode
      -- and for regression is statistical mean

- pasting: same as bagging but without replacement
- pasting should be preferred when you have large dataset as each predictor
  will be trained on independent data

- BaggingClassifier automatically performs soft voting if the predictor can predict probabilities

- out of bag evaluation (oob): evaluating each predictor of a bagging ensemble with
                               the leftover data that was not used for training

- random patches: sampling both training instances and features
- random subspaces: sampling only features
- bagging: sampling only instances



- random forest:
      -- bagging classifier with estimator = DecisionTree and max_samples = 1.0
      -- searches for the best feature to split from a random subset of features
      -- has better tree diversity than BaggingClassifier

- predictions made by random forest can be calculationally proven but are hard to interpret -> black box model

- extra trees:
      -- EXTremely RAndomized trees
      -- random forest does optimum split at each feature and then selects the feature with max information gain
      -- extra trees does random split at each feature and ...
      -- adds even more diversity to the ensemble

- 'feature_importances_' attribute of a tree like algorithm can tell us the importance
  of each feature in decision making



- boosting:
      -- train predictors sequentially, each predictor trying to correct its predecessor

- adaboost:
      -- adaptive boosting
      -- predictors are stumps by default (weak learner)
      -- each predictor has a different weight associated with their vote
            --- contradictory to random forest, where each predictor has same weight
      -- more the accuracy of predictor -> higher the weight to its vote
            --- for 0.5 accuracy -> 0 weight
            --- for less than 0.5 accuracy -> negative weight
      -- for the next predictor, sample weight of incorrectly classified instance is increases
         and sample weight of correctly classified instances is decreased. total sample weight again
         adds up to 1 (after normalizing)
      -- the new predictor now gives more emphasis on instances with high sample weights
      -- at last, the class with highest sum of predictor weights is the answer

- a decision tree with just one node and two leaves is called a stump

- gradient boosting:
      -- it starts by creating a single leaf
            -- with average value of target (regression)
      -- like adaboost gradient boost builds fixed sized trees (8-32 leaves)
      -- each tree tries to improve previous trees error
      -- 